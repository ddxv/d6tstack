{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering in Python with databolt  - Fast Loading to SQL with pandas (d6tlib/d6tstack)\n",
    "\n",
    "Pandas and SQL are great but they have some problems:\n",
    "* loading data from pandas to SQL is very slow. So you can't preprocess data with python and then quickly store it in a db\n",
    "* Loading CSV files into SQL is cumbersome and quickly breaks when input files are not consistent\n",
    "\n",
    "With `d6tstack` you can:\n",
    "* load pandas dataframes to postgres or mysql much faster than with `pd.to_sql()` and with minimal memory consumption\n",
    "* preprocess CSV files with pandas before writing to db\n",
    "* solve data schema problems (eg new or renamed columns) before writing to db \n",
    "* out of core functionality where large files are processed in chunks\n",
    "\n",
    "In this workbook we will demonstrate the usage of the d6tstack library for quickly loading data into SQL from CSV files and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd.to_sql() is slow\n",
    "Let's see how slow `pd.to_sql()` is storing 100k rows of random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import sqlalchemy\n",
    "import glob\n",
    "import time\n",
    "\n",
    "cfg_uri_psql = 'postgresql+psycopg2://psqlusr:psqlpwdpsqlpwd@localhost/psqltest'\n",
    "cfg_uri_mysql = 'mysql+mysqlconnector://testusr:testpwd@localhost/testdb'\n",
    "\n",
    "cfg_nobs = int(1e5)\n",
    "np.random.seed(0)\n",
    "df = pd.DataFrame({'id':range(cfg_nobs)})\n",
    "df['uuid']=[uuid.uuid4().hex.upper()[0:10] for _ in range(cfg_nobs)]\n",
    "df['date']=pd.date_range('1/1/2010',periods=cfg_nobs, freq='1T')\n",
    "for i in range(20):\n",
    "    df['d'+str(i)]=np.random.normal(size=int(cfg_nobs))\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlengine = sqlalchemy.create_engine(cfg_uri_psql)\n",
    "\n",
    "start_time = time.time()\n",
    "df.to_sql('benchmark',sqlengine,if_exists='replace')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up pd.to_sql() in postgres and mysql with d6tstack\n",
    "Let's see how we can make this faster. In this simple example we have a ~5x speedup with the speedup growing exponentially with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d6tstack.utils\n",
    "\n",
    "# psql\n",
    "start_time = time.time()\n",
    "d6tstack.utils.pd_to_psql(df, cfg_uri_psql, 'benchmark', if_exists='replace', sep='\\t')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# mysql\n",
    "start_time = time.time()\n",
    "d6tstack.utils.pd_to_mysql(df, cfg_uri_mysql, 'benchmark', if_exists='replace', sep='\\t')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas for preprocessing CSVs before storing to database\n",
    "Pandas is great for preprocessing data. For example lets say we want to process dates before importing them to a database. `d6tstack` makes this easy for you, you simply pass the filename or list of files along with the preprocessing function and it will be quickly loaded in SQL - without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_fname = 'test-data/input/test-data-input-csv-colmismatch-feb.csv'\n",
    "print(pd.read_csv(cfg_fname).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(dfg):\n",
    "    dfg['date'] = pd.to_datetime(dfg['date'], format='%Y-%m-%d')\n",
    "    dfg['date_year_quarter'] = (dfg['date'].dt.year).astype(str).str[-2:]+'Q'+(dfg['date'].dt.quarter).astype(str)\n",
    "    dfg['date_monthend'] = dfg['date'] + pd.tseries.offsets.MonthEnd()\n",
    "    return dfg\n",
    "\n",
    "d6tstack.combine_csv.CombinerCSV([cfg_fname], apply_after_read=apply,add_filename=False).to_psql_combine(cfg_uri_psql, 'benchmark', if_exists='replace')\n",
    "print(pd.read_sql_table('benchmark',sqlengine).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading multiple CSV to SQL with data schema changes\n",
    "Native database import commands only support one file. You can write a script to process multipe files which first of all is annoying and even worse it often breaks eg if there are schema changes. With `d6tstack` you quickly import multiple files and deal with data schema changes with just a couple of lines of python. The below is a quick example, to explore full functionality see  https://github.com/d6t/d6tstack/blob/master/examples-csv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import d6tstack.combine_csv\n",
    "\n",
    "cfg_fnames = list(glob.glob('test-data/input/test-data-input-csv-colmismatch-*.csv'))\n",
    "c = d6tstack.combine_csv.CombinerCSV(cfg_fnames)\n",
    "\n",
    "# check columns\n",
    "print('all equal',c.is_all_equal())\n",
    "print('')\n",
    "c.is_column_present()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of the additional `profit2` column in the 3rd file would break the data load. `d6tstack` will fix the situation and load everything correctly. And you can run any additional preprocessing logic like in the above example. All this is done out of core so you can process even large files without any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_fnames = list(glob.glob('test-data/input/test-data-input-csv-colmismatch-*.csv'))\n",
    "d6tstack.combine_csv.CombinerCSV(cfg_fnames, apply_after_read=apply,add_filename=False).to_psql_combine(cfg_uri_psql, 'benchmark', if_exists='replace')\n",
    "print(pd.read_sql_table('benchmark',sqlengine).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
